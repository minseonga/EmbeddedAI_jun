"""
ğŸš€ Hand Pose Estimation ëŒ€ì•ˆ ëª¨ë¸ + Pruning/Quantization ë²¤ì¹˜ë§ˆí¬

YOLO11-poseëŠ” torch_pruningê³¼ í˜¸í™˜ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
pruningì´ ê°€ëŠ¥í•œ ëŒ€ì•ˆ ëª¨ë¸ë“¤ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:

1. MobileNetV2 ê¸°ë°˜ Hand Keypoint ëª¨ë¸
2. MediaPipe Hands (ì´ë¯¸ ìµœì í™”ë¨)
3. ê°„ë‹¨í•œ CNN ê¸°ë°˜ Hand Pose ëª¨ë¸

í•µì‹¬: MobileNet/ResNetì€ torch_pruning 100% ì§€ì›!
"""

import os
import sys
import time
import copy
from pathlib import Path

import torch
import torch.nn as nn
import numpy as np

import torch_pruning as tp

try:
    from thop import profile
    HAS_THOP = True
except ImportError:
    HAS_THOP = False

ROOT = Path(__file__).resolve().parent
IMG_SIZE = 256  # Hand pose ëª¨ë¸ì€ ë³´í†µ ë” ì‘ì€ ì´ë¯¸ì§€ ì‚¬ìš©
NUM_KEYPOINTS = 21  # ì† 21ê°œ ê´€ì ˆ


# =========================================================
# Hand Pose ëª¨ë¸ ì •ì˜ (MobileNetV2 ê¸°ë°˜)
# =========================================================

class HandPoseNet(nn.Module):
    """
    MobileNetV2 Backbone + Hand Keypoint Head
    
    ì…ë ¥: (B, 3, 256, 256)
    ì¶œë ¥: (B, 21, 2) - 21ê°œ keypointì˜ x, y ì¢Œí‘œ
    """
    def __init__(self, num_keypoints=21, pretrained_backbone=True):
        super().__init__()
        
        # MobileNetV2 backbone
        from torchvision.models import mobilenet_v2, MobileNet_V2_Weights
        
        if pretrained_backbone:
            backbone = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)
        else:
            backbone = mobilenet_v2(weights=None)
        
        # featuresë§Œ ì‚¬ìš© (classifier ì œê±°)
        self.backbone = backbone.features  # (B, 1280, 8, 8) for 256x256 input
        
        # Keypoint head
        self.keypoint_head = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),  # (B, 1280, 1, 1)
            nn.Flatten(),              # (B, 1280)
            nn.Linear(1280, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_keypoints * 2),  # (B, 42) = 21 * 2
        )
        
        self.num_keypoints = num_keypoints
    
    def forward(self, x):
        features = self.backbone(x)  # (B, 1280, 8, 8)
        keypoints = self.keypoint_head(features)  # (B, 42)
        keypoints = keypoints.view(-1, self.num_keypoints, 2)  # (B, 21, 2)
        return keypoints


class HandPoseNetLite(nn.Module):
    """
    ê²½ëŸ‰ Hand Pose ëª¨ë¸ (MobileNetV3-Small ê¸°ë°˜)
    """
    def __init__(self, num_keypoints=21, pretrained_backbone=True):
        super().__init__()
        
        from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights
        
        if pretrained_backbone:
            backbone = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)
        else:
            backbone = mobilenet_v3_small(weights=None)
        
        self.backbone = backbone.features  # (B, 576, 8, 8)
        
        self.keypoint_head = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(576, 128),
            nn.ReLU(),
            nn.Linear(128, num_keypoints * 2),
        )
        
        self.num_keypoints = num_keypoints
    
    def forward(self, x):
        features = self.backbone(x)
        keypoints = self.keypoint_head(features)
        keypoints = keypoints.view(-1, self.num_keypoints, 2)
        return keypoints


# =========================================================
# Pruning í•¨ìˆ˜
# =========================================================

def prune_model(model, prune_ratio, ignored_layers=None):
    """
    torch_pruningìœ¼ë¡œ structured pruning
    """
    model = copy.deepcopy(model)
    example_inputs = torch.randn(1, 3, IMG_SIZE, IMG_SIZE)
    
    before_params = sum(p.numel() for p in model.parameters())
    
    # ignored_layers ê¸°ë³¸ê°’: ë§ˆì§€ë§‰ Linear ë ˆì´ì–´ (ì¶œë ¥ í¬ê¸° ìœ ì§€)
    if ignored_layers is None:
        ignored_layers = []
        for m in model.modules():
            if isinstance(m, nn.Linear):
                # ë§ˆì§€ë§‰ Linearë§Œ ë¬´ì‹œ
                ignored_layers = [m]
    
    try:
        pruner = tp.pruner.MagnitudePruner(
            model,
            example_inputs,
            importance=tp.importance.MagnitudeImportance(p=2),
            iterative_steps=1,
            pruning_ratio=prune_ratio,
            ignored_layers=ignored_layers,
            round_to=8,
        )
        
        pruner.step()
        
        after_params = sum(p.numel() for p in model.parameters())
        
        # Forward í…ŒìŠ¤íŠ¸
        with torch.no_grad():
            output = model(example_inputs)
        
        return model, before_params, after_params, True
        
    except Exception as e:
        print(f"   Pruning ì‹¤íŒ¨: {e}")
        return model, before_params, before_params, False


# =========================================================
# ë²¤ì¹˜ë§ˆí¬
# =========================================================

def benchmark(model, name, device='cpu'):
    model = model.to(device).eval()
    
    params = sum(p.numel() for p in model.parameters()) / 1e6
    
    flops = 0.0
    example = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)
    if HAS_THOP:
        try:
            macs, _ = profile(model, inputs=(example,), verbose=False)
            flops = macs / 1e9
            for m in model.modules():
                for attr in ['total_ops', 'total_params']:
                    if hasattr(m, attr):
                        delattr(m, attr)
        except:
            pass
    
    # ì†ë„ ì¸¡ì •
    fps = 0.0
    try:
        with torch.no_grad():
            for _ in range(10):
                model(example)
        
        times = []
        with torch.no_grad():
            for _ in range(50):
                t0 = time.time()
                model(example)
                times.append(time.time() - t0)
        
        fps = 1.0 / (sum(times) / len(times))
    except:
        pass
    
    return {
        'name': name,
        'params': params,
        'flops': flops,
        'fps': fps,
    }


def main():
    print("=" * 80)
    print("ğŸš€ Hand Pose ëŒ€ì•ˆ ëª¨ë¸ + Pruning ë²¤ì¹˜ë§ˆí¬")
    print("=" * 80)
    print("YOLO11-poseëŠ” torch_pruning í˜¸í™˜ X â†’ MobileNet ê¸°ë°˜ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    results = []
    
    # =========================================================
    # 1ï¸âƒ£ MobileNetV2 ê¸°ë°˜ Hand Pose
    # =========================================================
    print("\n[1] HandPoseNet (MobileNetV2 Backbone)...")
    
    model = HandPoseNet(num_keypoints=21, pretrained_backbone=True)
    result = benchmark(model, "HandPoseNet_MobileNetV2")
    results.append(result)
    print(f"   Params: {result['params']:.3f}M, FLOPs: {result['flops']:.3f}G, FPS: {result['fps']:.1f}")
    
    # Pruning í…ŒìŠ¤íŠ¸
    for ratio in [0.3, 0.5, 0.7]:
        pruned, before, after, success = prune_model(model, ratio)
        if success:
            result = benchmark(pruned, f"HandPoseNet_Pruned_{int(ratio*100)}%")
            results.append(result)
            print(f"   Pruned {int(ratio*100)}%: {result['params']:.3f}M, FLOPs: {result['flops']:.3f}G, FPS: {result['fps']:.1f}")
            
            # ì €ì¥
            save_path = ROOT / f"assets/models/handpose_mobilenetv2_pruned_{int(ratio*100)}.pt"
            torch.save(pruned.state_dict(), save_path)
    
    # =========================================================
    # 2ï¸âƒ£ MobileNetV3-Small ê¸°ë°˜ (ë” ê²½ëŸ‰)
    # =========================================================
    print("\n[2] HandPoseNetLite (MobileNetV3-Small Backbone)...")
    
    model_lite = HandPoseNetLite(num_keypoints=21, pretrained_backbone=True)
    result = benchmark(model_lite, "HandPoseNetLite_MobileNetV3")
    results.append(result)
    print(f"   Params: {result['params']:.3f}M, FLOPs: {result['flops']:.3f}G, FPS: {result['fps']:.1f}")
    
    # Pruning í…ŒìŠ¤íŠ¸
    for ratio in [0.3, 0.5, 0.7]:
        pruned, before, after, success = prune_model(model_lite, ratio)
        if success:
            result = benchmark(pruned, f"HandPoseNetLite_Pruned_{int(ratio*100)}%")
            results.append(result)
            print(f"   Pruned {int(ratio*100)}%: {result['params']:.3f}M, FLOPs: {result['flops']:.3f}G, FPS: {result['fps']:.1f}")
    
    # =========================================================
    # 3ï¸âƒ£ YOLO11n-pose (ë¹„êµìš©)
    # =========================================================
    print("\n[3] YOLO11n-pose (ë¹„êµìš©)...")
    try:
        from ultralytics import YOLO
        yolo_path = ROOT / "assets/models/yolo11n_hand_pose.pt"
        if yolo_path.exists():
            yolo = YOLO(yolo_path)
            
            yolo_params = sum(p.numel() for p in yolo.model.parameters()) / 1e6
            
            # YOLO ì†ë„ ì¸¡ì •
            dummy = np.zeros((640, 640, 3), dtype=np.uint8)
            for _ in range(10):
                yolo.predict(dummy, imgsz=640, verbose=False, device='cpu')
            
            times = []
            for _ in range(30):
                t0 = time.time()
                yolo.predict(dummy, imgsz=640, verbose=False, device='cpu')
                times.append(time.time() - t0)
            
            yolo_fps = 1.0 / (sum(times) / len(times))
            
            results.append({
                'name': 'YOLO11n-pose',
                'params': yolo_params,
                'flops': 3.96,  # ì´ì „ ì¸¡ì •ê°’
                'fps': yolo_fps,
            })
            print(f"   Params: {yolo_params:.3f}M, FLOPs: 3.96G, FPS: {yolo_fps:.1f}")
    except Exception as e:
        print(f"   YOLO ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # =========================================================
    # ê²°ê³¼ ë¹„êµ
    # =========================================================
    print("\n" + "=" * 90)
    print(f"{'Model':<40} | {'Params(M)':<12} | {'FLOPs(G)':<12} | {'FPS':<10}")
    print("-" * 90)
    for r in results:
        print(f"{r['name']:<40} | {r['params']:<12.3f} | {r['flops']:<12.3f} | {r['fps']:<10.1f}")
    print("=" * 90)
    
    print("\n" + "=" * 60)
    print("ğŸ“Š ë¶„ì„")
    print("=" * 60)
    print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… MobileNet ê¸°ë°˜ ëª¨ë¸ì˜ ì¥ì :                               â”‚
â”‚    â€¢ torch_pruning 100% ì§€ì›                                 â”‚
â”‚    â€¢ Structured Pruningìœ¼ë¡œ ì‹¤ì œ íŒŒë¼ë¯¸í„°/FLOPs ê°ì†Œ          â”‚
â”‚    â€¢ Quantization (TensorRT/TFLite) ì™„ë²½ ì§€ì›                 â”‚
â”‚    â€¢ ì†ë„ê°€ ë” ë¹ ë¦„ (256x256 ì…ë ¥)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âš ï¸ ê³ ë ¤ì‚¬í•­:                                                 â”‚
â”‚    â€¢ Hand detection í•„ìš” (YOLOëŠ” detection+keypoint í†µí•©)    â”‚
â”‚    â€¢ í•™ìŠµ ë°ì´í„° í•„ìš” (í˜„ì¬ëŠ” ImageNet pretrained backbone)   â”‚
â”‚    â€¢ YOLO11-pose ëŒ€ë¹„ ì •í™•ë„ í™•ì¸ í•„ìš”                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ’¡ ê¶Œì¥ íŒŒì´í”„ë¼ì¸:                                          â”‚
â”‚    â€¢ Hand Detection: YOLOv8n-detect (ì‘ì€ ëª¨ë¸)               â”‚
â”‚    â€¢ Hand Keypoint: MobileNet ê¸°ë°˜ ëª¨ë¸ (pruning ê°€ëŠ¥)        â”‚
â”‚    â€¢ ë˜ëŠ”: MediaPipe Hands (ì´ë¯¸ ìµœì í™”ë¨)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")


if __name__ == "__main__":
    main()
