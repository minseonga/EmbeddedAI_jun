"""
üñêÔ∏è MobileHand Pruning + Quantization Î≤§ÏπòÎßàÌÅ¨

MobileHand: https://github.com/gmntu/mobilehand
- MobileNetV3-Small backbone
- 21Í∞ú hand keypoint + 3D mesh  
- FreiHAND/STB Îç∞Ïù¥ÌÑ∞Î°ú ÌïôÏäµÎê®
- 3.82M params

Pruning/Quantization ÏôÑÏ†Ñ ÏßÄÏõê!
"""

import sys
sys.path.insert(0, 'mobilehand_repo/code')

import os
import time
import copy
from pathlib import Path

import torch
import torch.nn as nn
import numpy as np

import torch_pruning as tp

try:
    from thop import profile
    HAS_THOP = True
except ImportError:
    HAS_THOP = False

from utils_mobilenet_v3 import mobilenetv3_small
from utils_linear_model import LinearModel

ROOT = Path(__file__).resolve().parent
MODELS_DIR = ROOT / "assets/models"
PRETRAINED_PATH = ROOT / "mobilehand_repo/model/hmr_model_freihand_auc.pth"


# =========================================================
# MobileHand Î™®Îç∏ Ï†ïÏùò (Í∞ÑÏÜåÌôî Î≤ÑÏ†Ñ)
# =========================================================

class Regressor(LinearModel):
    def __init__(self, fc_layers, use_dropout, drop_prob, use_ac_func, num_param, num_iters, max_batch_size):
        super().__init__(fc_layers, use_dropout, drop_prob, use_ac_func)
        self.num_param = num_param
        self.num_iters = num_iters
        mean = np.zeros(self.num_param, dtype=np.float32)
        mean_param = np.tile(mean, max_batch_size).reshape((max_batch_size, -1))
        self.register_buffer('mean_param', torch.from_numpy(mean_param).float())
    
    def forward(self, inputs):
        bs = inputs.shape[0]
        param = self.mean_param[:bs, :]
        for _ in range(self.num_iters):
            total = torch.cat([inputs, param], dim=1)
            param = param + self.fc_blocks(total)
        return param


class MobileHandEncoder(nn.Module):
    """
    MobileHandÏùò Encoder Î∂ÄÎ∂ÑÎßå (Pruning Í∞ÄÎä•)
    
    ÏûÖÎ†•: (B, 3, 224, 224)
    Ï∂úÎ†•: (B, 576) - ÌäπÏßï Î≤°ÌÑ∞
    """
    def __init__(self):
        super().__init__()
        self.encoder = mobilenetv3_small()
    
    def forward(self, x):
        return self.encoder(x)
    
    def load_pretrained(self, path):
        """Pretrained weightÏóêÏÑú encoder Î∂ÄÎ∂ÑÎßå Î°úÎìú"""
        state_dict = torch.load(path, map_location='cpu')
        
        # encoder. Ï†ëÎëêÏÇ¨ Ï†úÍ±∞
        encoder_state = {}
        for k, v in state_dict.items():
            if k.startswith('encoder.'):
                encoder_state[k.replace('encoder.', '')] = v
        
        self.encoder.load_state_dict(encoder_state, strict=False)
        print(f"[Pretrained] Loaded encoder from {path}")


# =========================================================
# Pruning
# =========================================================

def prune_encoder(model, prune_ratio=0.5):
    """MobileNetV3 Encoder Pruning"""
    model = copy.deepcopy(model).cpu()  # CPUÏóêÏÑú pruning
    model.eval()
    
    example = torch.randn(1, 3, 224, 224)
    
    # Linear Î†àÏù¥Ïñ¥ Î¨¥Ïãú (Ï∂úÎ†• ÌÅ¨Í∏∞ Ïú†ÏßÄ)
    ignored = [m for m in model.modules() if isinstance(m, nn.Linear)]
    
    before = sum(p.numel() for p in model.parameters())
    
    pruner = tp.pruner.MagnitudePruner(
        model,
        example,
        importance=tp.importance.MagnitudeImportance(p=2),
        iterative_steps=1,
        pruning_ratio=prune_ratio,
        ignored_layers=ignored,
        round_to=8,
    )
    
    pruner.step()
    
    after = sum(p.numel() for p in model.parameters())
    
    print(f"[Pruning] {prune_ratio*100:.0f}%: {before/1e6:.3f}M ‚Üí {after/1e6:.3f}M ({(1-after/before)*100:.1f}%‚Üì)")
    
    return model


def get_flops(model, input_size=224):
    if not HAS_THOP:
        return 0
    
    model = copy.deepcopy(model).eval()
    dummy = torch.randn(1, 3, input_size, input_size)
    
    try:
        macs, _ = profile(model, inputs=(dummy,), verbose=False)
        for m in model.modules():
            for attr in ['total_ops', 'total_params']:
                if hasattr(m, attr):
                    delattr(m, attr)
        return macs / 1e9
    except:
        return 0


def measure_speed(model, input_size=224, device='cpu', num_test=100):
    model = model.to(device).eval()
    dummy = torch.randn(1, 3, input_size, input_size).to(device)
    
    # Warmup
    with torch.no_grad():
        for _ in range(20):
            model(dummy)
    
    # Measure
    times = []
    with torch.no_grad():
        for _ in range(num_test):
            t0 = time.time()
            model(dummy)
            if device == 'mps':
                torch.mps.synchronize()
            times.append(time.time() - t0)
    
    avg_time = sum(times) / len(times)
    return 1.0 / avg_time, avg_time * 1000


# =========================================================
# Î©îÏù∏
# =========================================================

def main():
    print("=" * 70)
    print("üñêÔ∏è MobileHand Pruning + Quantization Î≤§ÏπòÎßàÌÅ¨")
    print("=" * 70)
    
    # Device
    if torch.backends.mps.is_available():
        device = 'mps'
    elif torch.cuda.is_available():
        device = 'cuda'
    else:
        device = 'cpu'
    print(f"Device: {device}")
    
    results = []
    
    # =========================================================
    # 1. Original MobileHand Encoder
    # =========================================================
    print("\n[1] Original MobileHand Encoder...")
    
    model = MobileHandEncoder()
    
    if PRETRAINED_PATH.exists():
        model.load_pretrained(PRETRAINED_PATH)
    
    params = sum(p.numel() for p in model.parameters()) / 1e6
    flops = get_flops(model)
    fps, latency = measure_speed(model, device=device)
    
    print(f"   Params: {params:.3f}M, FLOPs: {flops:.3f}G, FPS: {fps:.1f}")
    
    results.append({
        'name': 'MobileHand_Original',
        'params': params,
        'flops': flops,
        'fps': fps,
        'latency': latency,
    })
    
    # Ï†ÄÏû•
    save_path = MODELS_DIR / "mobilehand_encoder.pt"
    torch.save(model.state_dict(), save_path)
    
    # =========================================================
    # 2. Pruning
    # =========================================================
    print("\n[2] Pruning...")
    
    for ratio in [0.3, 0.5, 0.7]:
        pruned = prune_encoder(model, prune_ratio=ratio)
        
        p_params = sum(p.numel() for p in pruned.parameters()) / 1e6
        p_flops = get_flops(pruned)
        p_fps, p_lat = measure_speed(pruned, device=device)
        
        print(f"   Params: {p_params:.3f}M, FLOPs: {p_flops:.3f}G, FPS: {p_fps:.1f}")
        
        results.append({
            'name': f'MobileHand_Pruned_{int(ratio*100)}%',
            'params': p_params,
            'flops': p_flops,
            'fps': p_fps,
            'latency': p_lat,
        })
        
        # Ï†ÄÏû•
        save_pruned = MODELS_DIR / f"mobilehand_encoder_pruned_{int(ratio*100)}.pt"
        torch.save(pruned.state_dict(), save_pruned)
    
    # =========================================================
    # 3. ONNX Export
    # =========================================================
    print("\n[3] ONNX Export...")
    
    model.cpu().eval()
    dummy = torch.randn(1, 3, 224, 224)
    
    onnx_path = MODELS_DIR / "mobilehand_encoder.onnx"
    torch.onnx.export(model, dummy, str(onnx_path),
                     input_names=['input'], output_names=['features'],
                     opset_version=11)
    print(f"   Saved: {onnx_path.name}")
    
    # Pruned 50% ONNX
    pruned_50 = prune_encoder(model, 0.5)
    onnx_pruned = MODELS_DIR / "mobilehand_encoder_pruned_50.onnx"
    torch.onnx.export(pruned_50.cpu(), dummy, str(onnx_pruned),
                     input_names=['input'], output_names=['features'],
                     opset_version=11)
    print(f"   Saved: {onnx_pruned.name}")
    
    # =========================================================
    # Í≤∞Í≥º
    # =========================================================
    print("\n" + "=" * 80)
    print(f"{'Model':<30} | {'Params(M)':<12} | {'FLOPs(G)':<12} | {'FPS':<10} | {'Latency(ms)':<12}")
    print("-" * 80)
    for r in results:
        print(f"{r['name']:<30} | {r['params']:<12.3f} | {r['flops']:<12.3f} | {r['fps']:<10.1f} | {r['latency']:<12.1f}")
    print("=" * 80)
    
    print(f"""
‚úÖ ÏôÑÎ£å!

üìÅ ÏÉùÏÑ±Îêú ÌååÏùº:
   ‚Ä¢ mobilehand_encoder.pt - Pretrained encoder
   ‚Ä¢ mobilehand_encoder_pruned_*.pt - Pruned Î™®Îç∏
   ‚Ä¢ mobilehand_encoder.onnx - TensorRTÏö©
   ‚Ä¢ mobilehand_encoder_pruned_50.onnx - Pruned ONNX

üöÄ Jetson TensorRT:
   trtexec --onnx=mobilehand_encoder_pruned_50.onnx --fp16 --saveEngine=mobilehand_fp16.engine

üí° Ïû•Ï†ê:
   ‚Ä¢ Ïù¥ÎØ∏ FreiHANDÎ°ú ÌïôÏäµÎê® (Ïû¨ÌïôÏäµ Î∂àÌïÑÏöî!)
   ‚Ä¢ Pruning + Quantization 100% ÏßÄÏõê
   ‚Ä¢ YOLO11-poseÎ≥¥Îã§ Í∞ÄÎ≥çÍ≥† Îπ†Î¶Ñ
""")


if __name__ == "__main__":
    main()
